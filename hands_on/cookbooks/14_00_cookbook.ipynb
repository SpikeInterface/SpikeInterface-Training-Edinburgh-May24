{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spikeinterface Data Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Quote](./quote.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Object of Spikeinterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not JUST numpy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "![Numpy is great](./numpy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because the data in electrophysiology is TOO MASSIVE for most users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 384\n",
    "sampling_frequency = 30_000.0 # Hz\n",
    "\n",
    "total_time_hours = 24 \n",
    "total_time_seconds = total_time_hours * 60 * 60\n",
    "\n",
    "num_samples = int(total_time_seconds * sampling_frequency)\n",
    "num_channels, num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.rand(num_channels, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording objects in Spikeinterface\n",
    "* Keep a reference to the data without loading it into memory (memmaps)\n",
    "* Keep a set of human readable ids to refer to the channels\n",
    "* Keep a sampling frequency to transform samples to times\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Recording](./recording.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import generate_recording \n",
    "\n",
    "recording = generate_recording(num_channels=3, durations=[10])\n",
    "recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting channel names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = recording.rename_channels(new_channel_ids=[\"a\", \"b\", \"c\"])  # This is not in-place\n",
    "recording.get_channel_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording.get_traces(start_frame=0, end_frame=3, channel_ids=[\"a\", \"c\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting pieces of the recording (lazily!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_sliced_recording = recording.select_channels(channel_ids=[\"a\", \"b\"])\n",
    "channel_sliced_recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frames / Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_recording = recording.frame_slice(start_frame=0, end_frame=1000)\n",
    "sliced_recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating recordings (across time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import concatenate_recordings\n",
    "\n",
    "recording1 = generate_recording(num_channels=3, durations=[10])\n",
    "recording2 = generate_recording(num_channels=3, durations=[10])\n",
    "\n",
    "concanted_recordings = concatenate_recordings([recording1, recording2])\n",
    "\n",
    "assert concanted_recordings.get_duration() == recording1.get_duration()  + recording2.get_duration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating channels as a single recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import aggregate_channels\n",
    "\n",
    "recording1 = generate_recording(num_channels=3, durations=[10], set_probe=False)  # To avoid location check\n",
    "recording1 = recording1.rename_channels(new_channel_ids=[\"a\", \"b\", \"c\"])\n",
    "recording2 = generate_recording(num_channels=2, durations=[10], set_probe=False)  \n",
    "recording2 = recording2.rename_channels(new_channel_ids=[\"d\", \"e\"])\n",
    "\n",
    "aggregated_recording = aggregate_channels([recording1, recording2])  \n",
    "assert aggregated_recording.get_num_channels() == 5\n",
    "assert list(aggregated_recording.get_channel_ids()) == ['a', 'b', 'c', 'd', 'e']  # Failing right now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Summary of Recording Operations\n",
    "\n",
    "![Recording Operations](./recording_operations.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sorting objects in Spikeinterface\n",
    "\n",
    "In their most common representation (as loaded by the user), sorting objects are a dictionary of spike trains (represented as frames) and a sampling frequency to situate those frames in time\n",
    "\n",
    "![Sorting](./sorting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import generate_sorting \n",
    "\n",
    "sorting = generate_sorting(num_units=3, durations=[10])\n",
    "sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the unit ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = sorting.rename_units(new_unit_ids=[\"unit1\", \"unit2\", \"unit3\"])  # This is not in-place\n",
    "sorting.get_unit_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting.get_unit_spike_train(unit_id=\"unit2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting pieces of the sorting (lazily!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_selected_sorting = sorting.select_units(unit_ids=[\"unit1\", \"unit2\"])\n",
    "unit_selected_sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frames / Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_sorting = sorting.frame_slice(start_frame=0, end_frame=1000)\n",
    "sliced_sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sortings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating sorting objects (across time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import generate_sorting, concatenate_sortings\n",
    "\n",
    "\n",
    "duration = 1.0 # Seconds\n",
    "sorting1 = generate_sorting(num_units=3, durations=[duration], seed=0)\n",
    "sorting1 = sorting1.rename_units(new_unit_ids=[\"unit1\", \"unit2\", \"unit3\"])\n",
    "sorting2 = generate_sorting(num_units=3, durations=[duration], seed=1)\n",
    "sorting2 = sorting2.rename_units(new_unit_ids=[\"unit1\", \"unit2\", \"unit3\"])\n",
    "\n",
    "num_samples_sorting1 = sorting1.sampling_frequency * duration\n",
    "num_samples_sorting2 = sorting2.sampling_frequency * duration\n",
    "\n",
    "concatenated_sorting = concatenate_sortings(sorting_list=[sorting1, sorting2], total_samples_list=[num_samples_sorting1, num_samples_sorting2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting1.get_unit_spike_train(unit_id=\"unit1\", return_times=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting2.get_unit_spike_train(unit_id=\"unit1\", return_times=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting2.get_unit_spike_train(unit_id=\"unit1\", return_times=True) + duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_sorting.get_unit_spike_train(unit_id=\"unit1\", return_times=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating units as a single sorting object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import generate_sorting, aggregate_units\n",
    "\n",
    "\n",
    "duration = 10.0 # Seconds\n",
    "sorting1 = generate_sorting(num_units=3, durations=[duration])\n",
    "sorting2 = generate_sorting(num_units=2, durations=[duration])\n",
    "\n",
    "\n",
    "aggregated_sorting = aggregate_units([sorting1, sorting2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Summary of Sorting Operations\n",
    "\n",
    "\n",
    "![Sorting Operations](./sorting_operations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick reminder about recording\n",
    "\n",
    "![Recording](./recording.png)\n",
    "\n",
    "### What do we parallelize over\n",
    "\n",
    "![Chuking Description](./parallel_processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to control paralell execution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lenght of the chunk\n",
    "    * chunk_duration :  Lenght of the chunk in seconds\n",
    "    * chunk_size: Number of samples per chunk\n",
    "    * chunk_memory: Memory usage for each job\n",
    "    * total_memory Total memory usage \n",
    "* n_jobs: Number of jobs to use. With -1 the number of jobs is the same as number of cores\n",
    "* progress_bar: Whether to show a progress bar\n",
    "* mp_context: fork, span or forkserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import generate_recording, write_binary_recording\n",
    "import tempfile\n",
    "\n",
    "recording = generate_recording(num_channels=3, durations=[10])\n",
    "\n",
    "\n",
    "job_kwargs = {\"n_jobs\":2, \"chunk_duration\": 5.0, \"progress_bar\": 1, 'progress_bar': True}\n",
    "\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.raw', delete=False) as temp_file:\n",
    "    temporary_file_path = temp_file.name\n",
    "    write_binary_recording(recording=recording, file_paths=[temporary_file_path], **job_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job_kwargs](./job_kwargs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting global job_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface import set_global_job_kwargs, get_global_job_kwargs\n",
    "from spikeinterface.core import generate_recording, write_binary_recording\n",
    "import tempfile\n",
    "\n",
    "recording = generate_recording(num_channels=3, durations=[10])\n",
    "\n",
    "\n",
    "print(get_global_job_kwargs())\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.raw', delete=False) as temp_file:\n",
    "    temporary_file_path = temp_file.name\n",
    "    write_binary_recording(recording=recording, file_paths=[temporary_file_path])\n",
    "\n",
    "\n",
    "set_global_job_kwargs(n_jobs=2, chunk_duration=5.0, progress_bar=True)\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.raw', delete=False) as temp_file:\n",
    "    temporary_file_path = temp_file.name\n",
    "    write_binary_recording(recording=recording, file_paths=[temporary_file_path])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop to mentioned cautionary tales:\n",
    "* Performance is highly dependent on the data, operations and the hardware.\n",
    "* The best way to optimize is to try different configurations and see what works best for your data and hardware.\n",
    "* Threading vs multiprocessing: Threading is generally faster for I/O bound tasks, while multiprocessing is better for CPU bound tasks.\n",
    "* Anything to add?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spike Vector](./spike_vector.png)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import generate_sorting \n",
    "\n",
    "sorting = generate_sorting(num_units=3, durations=[10])\n",
    "sorting.to_spike_vector(concatenated=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Common Chunk](./common_chunk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Recording and Sorting Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from spikeinterface.core import generate_recording\n",
    "\n",
    "recording = generate_recording(num_channels=3, durations=[10], set_probe=True)\n",
    "recording = recording.rename_channels(new_channel_ids=[\"a\", \"b\", \"c\"])  # This is not in-place\n",
    "recording.set_property(\"a_property\", [\"value1\", \"value2\", \"value3\"])  # This is in place\n",
    "\n",
    "\n",
    "\n",
    "job_kwargs={'progress_bar': True, \"verbose\":True, \"n_jobs\":2}\n",
    "\n",
    "folder_path = Path(\"./test_recording\")\n",
    "\n",
    "\n",
    "binary_recording = recording.save_to_folder(folder=folder_path,  overwrite=True, **job_kwargs)   \n",
    "binary_recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from spikeinterface.core import generate_sorting\n",
    "\n",
    "sorting = generate_sorting(num_units=3, durations=[10])\n",
    "sorting = sorting.rename_units(new_unit_ids=[\"unit1\", \"unit2\", \"unit3\"])  # This is not in-place\n",
    "\n",
    "sorting.set_property(\"a_property\", [\"value1\", \"value2\", \"value3\"])  # This is in place\n",
    "\n",
    "\n",
    "folder_path = Path(\"./test_sorting\")\n",
    "\n",
    "job_kwargs={'progress_bar': True, \"verbose\":True, \"n_jobs\":2}\n",
    "binary_sorting = sorting.save_to_folder(folder=folder_path,  overwrite=True, **job_kwargs)   \n",
    "binary_sorting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zarr format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from spikeinterface.core import generate_recording\n",
    "\n",
    "recording = generate_recording(num_channels=3, durations=[10], set_probe=True)\n",
    "recording = recording.rename_channels(new_channel_ids=[\"a\", \"b\", \"c\"])  # This is not in-place\n",
    "recording.set_property(\"a_property\", [\"value1\", \"value2\", \"value3\"])  # This is in place\n",
    "\n",
    "folder_path = Path(\"./test_recording.zarr\")\n",
    "\n",
    "job_kwargs={'progress_bar': True, \"verbose\":True, \"n_jobs\":2}\n",
    "zarr_recording = recording.save_to_zarr(folder=folder_path,  overwrite=True, **job_kwargs)   \n",
    "zarr_recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_recording._root.tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from spikeinterface.core import generate_sorting\n",
    "\n",
    "sorting = generate_sorting(num_units=3, durations=[10])\n",
    "sorting = sorting.rename_units(new_unit_ids=[\"unit1\", \"unit2\", \"unit3\"])  # This is not in-place\n",
    "\n",
    "sorting.set_property(\"a_property\", [\"value1\", \"value2\", \"value3\"])  # This is in place\n",
    "\n",
    "\n",
    "folder_path = Path(\"./test_sorting.zarr\")\n",
    "\n",
    "job_kwargs={'progress_bar': True, \"verbose\":True, \"n_jobs\":2}\n",
    "zarr_sorting = sorting.save_to_zarr(folder=folder_path,  overwrite=True, **job_kwargs)   \n",
    "zarr_sorting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_sorting._root.tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving is portable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from spikeinterface.core import generate_recording\n",
    "from spikeinterface.core import load_extractor  \n",
    "\n",
    "recording = generate_recording(num_channels=3, durations=[10], set_probe=True)\n",
    "\n",
    "\n",
    "# Save a recording within a nested folder\n",
    "base_folder = Path.cwd() / \"saving_recording_and_moving\" \n",
    "original_recording_folder = base_folder / \"folderA\" / \"recording_folder\"  \n",
    "\n",
    "recording = recording.save_to_folder(folder=original_recording_folder, overwrite=True)\n",
    "\n",
    "\n",
    "# We move the folder from its original location one level up to the current folder\n",
    "another_folder = base_folder / \"folderB\"\n",
    "another_folder.mkdir(exist_ok=True)\n",
    "destination_folder = another_folder / \"recording_folder\"\n",
    "\n",
    "original_recording_folder.rename(destination_folder)\n",
    "\n",
    "\n",
    "\n",
    "load_extractor(file_or_folder_or_dict=destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving preprocessing provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from spikeinterface.core import generate_recording, load_extractor\n",
    "from spikeinterface.preprocessing import bandpass_filter, common_reference\n",
    "\n",
    "# First we simulate having raw data (can be large!) in a folder \n",
    "simulated_recording = generate_recording(num_channels=3, durations=[10])\n",
    "base_folder = Path.cwd() / \"working_with_preprocessing_provenance\"\n",
    "\n",
    "raw_data_location = base_folder/ \"raw_data_location_folder\"\n",
    "raw_data_location.mkdir(parents=True, exist_ok=True)\n",
    "recording_saved = simulated_recording.save(folder=raw_data_location, overwrite=True)\n",
    "\n",
    "\n",
    "# Now our common analysis pipeline starts by loading the raw data\n",
    "raw_data_recording = load_extractor(file_or_folder_or_dict=raw_data_location)\n",
    "# And then we apply some preprocessing steps\n",
    "recording_preprocessed = bandpass_filter(common_reference(raw_data_recording))\n",
    "\n",
    "# Note that we can save our preprocessed data for faster access afterwards:\n",
    "# recording_preprocessed.save(folder=Path.cwd() / \"preprocessed_data_folder\")\n",
    "# But maybe we only want to save our provenance data, that is, our pre-processing pipeline and parameters\n",
    "# For this we can save oure preprocessing pipeline as a json file\n",
    "\n",
    "# dump_to_json without relative_to\n",
    "\n",
    "json_file_path = base_folder / \"analysis_folder\" / \"preprocessed_pipeline.json\"\n",
    "recording_preprocessed.dump_to_json(file_path=json_file_path)\n",
    "\n",
    "\n",
    "json_file_path_relative = base_folder / \"analysis_folder\" / \"preprocessed_pipeline_relative.json\"\n",
    "recording_preprocessed.dump_to_json(file_path=json_file_path_relative, relative_to=base_folder)\n",
    "\n",
    "\n",
    "# This then can be loaded again to recover the pipeline\n",
    "recovered_pipeline = load_extractor(file_or_folder_or_dict=json_file_path)\n",
    "\n",
    "# We move the json to a new folder\n",
    "new_base_folder = Path.cwd() / \"new_working_with_preprocessing_provenance\"\n",
    "new_base_folder.mkdir(exist_ok=True)\n",
    "\n",
    "base_folder.rename(new_base_folder)\n",
    "\n",
    "new_json_file_path = new_base_folder / \"analysis_folder\" / \"preprocessed_pipeline.json\"\n",
    "assert new_json_file_path.is_file(), \"The json file was not moved correctly\" \n",
    "\n",
    "# Try to load it\n",
    "try:\n",
    "    recovered_pipeline = load_extractor(file_or_folder_or_dict=new_json_file_path)\n",
    "except:\n",
    "    print(\"This generated an error because the references to the raw data were absolute\")\n",
    "    \n",
    "# We can solve this by saving the json file with relative paths\n",
    "\n",
    "new_json_file_path_with_relative = new_base_folder / \"analysis_folder\" / \"preprocessed_pipeline_relative.json\"\n",
    "assert new_json_file_path_with_relative.is_file(), \"The json file with relative was not moved correctly\"\n",
    "recovered_pipeline = load_extractor(file_or_folder_or_dict=new_json_file_path_with_relative, base_folder=new_base_folder)\n",
    "recovered_pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
